# src/train_swing_model.py

import warnings
warnings.filterwarnings("ignore", message=".*gpu_hist.*")
warnings.filterwarnings("ignore", message=".*use_label_encoder.*")

import pandas as pd
import numpy as np
from datetime import timedelta
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, KFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
import joblib

# ─── 1) LOAD RAW TICK‐LEVEL OR MINUTE DB ────────────────────────────────────────
# assume you have a minute‐level CSV; we'll resample to 1h
df = pd.read_csv(
    "data/technical_indicators.csv",
    parse_dates=["timestamp"], index_col="timestamp"
)

# ─── 2) RESAMPLE TO 1H SWING CANDLES ─────────────────────────────────────────
ohlc = {
    "open":"first", "high":"max", "low":"min",
    "close":"last", "volume":"sum"
}
df1h = df.resample("1H").agg(ohlc).dropna()

# ─── 3) FEATURE ENGINEERING FOR SWING TRADES ─────────────────────────────────
df1h["MA50"]   = df1h["close"].rolling(50).mean()
df1h["MA200"]  = df1h["close"].rolling(200).mean()

# RSI14
delta = df1h["close"].diff()
up = delta.clip(lower=0).rolling(14).mean()
dn = -delta.clip(upper=0).rolling(14).mean()
df1h["RSI14"] = 100 - 100/(1 + up/dn)

# VWAP
df1h["VWAP"] = (df1h["close"] * df1h["volume"]).cumsum() / df1h["volume"].cumsum()

# 20‐bar swing highs & lows
df1h["swing_high20"] = df1h["high"].rolling(20).max().shift(1)
df1h["swing_low20"]  = df1h["low"].rolling(20).min().shift(1)

# Volume spike: vol / 20‐bar avg vol
df1h["vol_spike"] = df1h["volume"] / df1h["volume"].rolling(20).mean()

df1h.dropna(inplace=True)

# ─── 4) LOAD PRE-COMPUTED LABELS ──────────────────────────────────────────────
#    (these are generated by `src/label_generator.py`)
labels_df = pd.read_csv(
    "data/swing_labeled_dataset.csv",
    parse_dates=["timestamp"], index_col="timestamp"
)
df1h = df1h.join(labels_df["label"], how="inner")
df1h.dropna(inplace=True)  # drop rows where label might be NaN
df1h = df1h[df1h["label"] != 0] # only train on valid trade signals

# ─── 5) PREPARE TRAIN / TARGET ─────────────────────────────────────────────────
feature_cols = [c for c in df1h.columns if c not in ["label", "open", "high", "low", "close", "volume"]]
X = df1h[feature_cols]
y = df1h["label"].map({-1:0, 1:1})  # map to binary for classifier

# Calculate scale_pos_weight
scale_pos_weight = (y == 0).sum() / (y == 1).sum()

# ─── 6) TIME-SERIES CV & ENSEMBLE MODELING ───────────────────────────────────
tscv = TimeSeriesSplit(n_splits=3)

# Base models
xgb = XGBClassifier(
    tree_method="gpu_hist",      # ← GPU histogram builder
    predictor="gpu_predictor",   # ← GPU predictor for inference
    gpu_id=0,                    # ← first GPU device
    eval_metric="logloss",
    scale_pos_weight=scale_pos_weight,
    random_state=42
)
lgbm = LGBMClassifier(
    device="gpu",               # ← run on GPU
    gpu_platform_id=0,          # ← first CUDA platform (usually 0)
    gpu_device_id=0,            # ← first GPU device
    random_state=42
)
rf = RandomForestClassifier(random_state=42)

# Stacking classifier
estimators = [
    ('xgb', xgb),
    ('lgbm', lgbm),
    ('rf', rf)
]
stack = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(),
    cv=KFold(n_splits=5)
)

# Hyperparameter search for the whole stack
param_dist = {
    'xgb__n_estimators': [100, 200, 300],
    'xgb__learning_rate': [0.01, 0.05, 0.1],
    'xgb__max_depth': [3, 5, 7],
    'lgbm__n_estimators': [100, 200, 300],
    'lgbm__learning_rate': [0.01, 0.05, 0.1],
    'lgbm__num_leaves': [20, 31, 40],
    'rf__n_estimators': [100, 200, 300],
    'rf__max_depth': [10, 20, 30],
}

search = RandomizedSearchCV(
    estimator=stack,
    param_distributions=param_dist,
    n_iter=10,
    cv=tscv,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42,
    verbose=1,
    error_score='raise'
)
search.fit(X, y)

print("▶️ Best params:", search.best_params_)
print("▶️ Best ROC-AUC:", search.best_score_)

best_model = search.best_estimator_

# ─── 7) WALK-FORWARD VALIDATION ───────────────────────────────────────────────
tscv_walk_forward = TimeSeriesSplit(n_splits=5)
all_preds = []
all_ys = []

for train_idx, test_idx in tscv_walk_forward.split(X):
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

    # Fit the model on the training data
    best_model.fit(X_train, y_train)

    # Make predictions on the test data
    y_proba = best_model.predict_proba(X_test)[:, 1]
    
    # Threshold tuning for this fold
    best_f1 = 0
    best_thresh = 0.5
    for thresh in np.arange(0.3, 0.7, 0.01):
        y_pred_tuned = (y_proba > thresh).astype(int)
        if y_test.sum() > 0 and (y_pred_tuned.sum() > 0): # Ensure there are positive samples in both
            f1 = classification_report(y_test, y_pred_tuned, output_dict=True, zero_division=0).get('1', {}).get('f1-score', 0)
            if f1 > best_f1:
                best_f1 = f1
                best_thresh = thresh
        
    y_pred = (y_proba > best_thresh).astype(int)

    all_preds.extend(y_pred)
    all_ys.extend(y_test)

print("\n--- Walk-Forward Validation Results ---")
if not all_ys:
    print("No test samples were generated in walk-forward. Check your split configuration.")
else:
    print(classification_report(all_ys, all_preds, labels=[0, 1], target_names=["Short", "Long"], zero_division=0))
    print("Confusion Matrix:")
    print(confusion_matrix(all_ys, all_preds))
    print("Overall ROC-AUC:", roc_auc_score(all_ys, all_preds))

# ─── 8) SAVE THE SWING MODEL ──────────────────────────────────────────────────
if 'best_model' in locals():
    joblib.dump(best_model, "models/xgb_swing_labeler.pkl")
    print("✅ Saved swing-trade model to models/xgb_swing_labeler.pkl")
else:
    print("⚠️  No best model to save—no walk-forward folds ran.")
